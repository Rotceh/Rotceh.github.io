<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="en-us" />
<meta name="version" content="3156"/>
<style>body {font-family:Helvetica}ul.comment{list-style-image:url('https://hackpad.com/static/img/comment.png');} ul.task{list-style-image:url('https://hackpad.com/static/img/unchecked.png');}ul.taskdone{list-style-image:url('https://hackpad.com/static/img/checked.png');} </style><title>/21841$Dp5GEmGaPmf</title>
</head>
<body><h1>Stanford Machine Learning by Andrew Ng on Coursera</h1><p></p><h2>Lecture I 介紹.</h2><p><b>What is machine learning?</b></p><p>1998s 卡內基梅隆的Tom Mitchell 對ML所下的定義：</p><p>一個computer program被認為能從<b><i>經驗E </i></b>中學習，解決<b><i>任務T</i></b>，達到一個<b><i>效能P </i></b>。</p><p></p><p>從關於Ｔ中去學習Ｅ，之後用這個Ｅ<i>(Learning model) </i>去執行Ｔ會得到效能值Ｐ，如果Ｐ不理想的話，則可以想辦法去改善Ｅ。</p><p></p><p><b>區分machine learning algorithms</b></p><ul><li>Supervised 監督式學習</li>
<li>Unsupervised 非監督式學習</li>
<li>Others: Reinforcement learning(加強式學習)、recommender systems(推薦系統)</li></ul>

<h2>Lecture V Octave Tutorial</h2><p>Octave 是GNU的一套數學軟體類似 Matlab，不過Matlab是要錢的;Octave則是開源軟體。語法基本上與matlab相同，但兩者的toolbox並不相容。</p><p>你會問說怎麼不用programming Language like: C++ 、Java、python</p><p>來實作演算法呢？</p><p>那是因為據以往經驗，你必需要對語言非常熟悉，否則在實作的過程中，常常會陷在一些在coding上的錯誤，一直除錯。這樣一來便失去學習機器學習演算法的意義了。因此，我們選一個所謂的prototype language 來實作演算法時，會比較好上手也比較省時間。</p><p>往後在開發有關機器學習的應用時，不彷也先用octave或matlab built 出一個prototype測試看看可行性，覺得沒問題再用programming language 去完成他。</p><p></p><ul class="comment"><li>這一章基本上都是在講授Octave的基本使用方式，有興趣自行研究。</li></ul>

<ul class="comment"><li>GNU是一堆自由軟體的集成，由Richard stallman提出的</li></ul>

<h2>Lecture VI Logistic Regression(邏輯式回歸)</h2><p><b>Classification 分類問題</b></p><ul><li>&nbsp; 二元分類 y={0,1}&nbsp; 通常假定1為positive 正樣本&nbsp;&nbsp;&nbsp; 0為negative 負樣本</li>
<li>&nbsp; 多分類 y={0,1,2,3....}&nbsp;</li></ul>

<p>以分類是否為惡性腫瘤為例</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421830674709_c1.png'/></p><p>看起來似乎用<b>Linear regression</b>是可得到不錯的結果，真的嗎？</p><p>那假如我又得到一筆新的資料，下圖最右邊的x，會使得我們得到一個不好的 hypothesis。所以通常不要用線性回歸去解分類的問題。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421831636892_c1.png'/></p><p><b>且用線性回歸去解分類問題，會造成一個現象</b></p><p><b>你要輸出的 y = 0 or 1</b></p><p><b>但是hypothesis 吐出來的結果可能會 &gt;1 or &lt;0 ，這顯然有問題。</b></p><p></p><p>為了解決這個現象，有人提出了<b>Logistic Regression</b>邏輯回歸演算法</p><p>請注意：這是一個解決<b>分類問題</b>的算法，別被回歸這個字給影響。</p><p></p><p><b>假設之表示式 Hypothesis Representation&nbsp;</b></p><p>Hypothesis 為 Sigmoid function or Logistic function</p><p>如圖右下方 當z值趨近正無限大 y會非常的接近1;反之則接近0。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421843990526_c1.png'/></p><p></p><p>Hypothesis輸出的正確解釋為這樣</p><p>當你給定一個特徵向量x=[ x1 , x2]=[ 1 , tumorSize] 則 <i>h( X) =0.7</i></p><p>這告訴我們病患有70％的機率為惡性腫瘤</p><p>如果用正規數學式來表示的話，就是用條件機率</p><p>給定x的條件下y = 1的機率式多少？</p><p>你會想問why？為什麼條件機率有關？</p><p>前面粗體的地方提到<b>你要輸出的 y = 0 or 1&nbsp;</b></p><p><b>但是用線性回歸的hypothesis 吐出來的結果可能會 &gt;1 or &lt;0&nbsp;</b></p><p>因此，用機率可以讓出來的值介於0～1之間，了解否？</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421845471381_c1.png'/></p><p></p><p><b>決策邊界 Decision boundary&nbsp;</b></p><p>下圖右邊polynomial hypothesis的參數theta先假定已經擬合好，</p><p>分別為 -3, 1 , 1</p><p>到目前為止，我們尚未談到如何去fitting(擬合)模型中的參數，下一段視頻我們會談到。</p><p>這些參數的選擇是為了讓我們試著找出hypothesis何時將預測y=1,何時=0。</p><p>圖中不等式將-3移向，得到藍筆的部份。告訴我們當你輸入的data滿足藍色不等式的時候，就會預測y=1。</p><p>我們將藍筆不等式改寫成粉紅色的等式，很顯然這是一條直線方程式。</p><p>也就是所謂的Decision boundary。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421979310572_c1.png'/></p><ul style="list-style: none;"><li>( 圖：o為負樣本&nbsp; x為正樣本 )</li></ul>
<p>這裡有一點要注意的是</p><p><b>Decision boundary 是這個hypothesis的一個性質property，與原始的data無關。</b></p><p>也就是說，當給定的data與上圖的分佈是不同情況時，我們就需要去修改hypothesis的參數－參數擬合的問題。</p><p>e.g.下圖data的分佈情況，不為線性可分。所以得修改hypothesis，找出合適的Decision boundary。</p><p>可以明顯的看出Decision boundary為一個單位圓。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421982264614_c1.png'/></p><p>再次強調我們不是用training set來定義Decision boundary，而是用training set來擬合(fitting) 參數theta。一旦找到這些參數，他就決定了Decision boundary。</p><p>當然啦！你的data分佈可能會有更複雜的情況，會需要用到更高階的多項式來做為hypothesis，出來的Decision boundary可能會是橢圓、甚至其他有趣的圖形。</p><p></p><p><b>成本函數或代價函數 cost function</b></p><p>顧名思義要在hypothesis 預測出的結果h(x) 與實際結果y之間找出要付出的成本是多少。以線性回歸來講，他的cost function即為兩者差值取平方。</p><p>這裡我們要做的就是盡可能讓cost function最小化。假使我們用兩者差值取平方來做為邏輯式回歸的cost function會使得參數theta造出來的function為非凸函數non-convex。</p><p>什麼是非凸函數呢？簡單講他不會是一個單調遞增的函數。</p><p>而且在非凸函數上使用梯度下降法，我們無法保證一定能找到Global minimum，這也是sigmoid本身就是一個複雜的非線性函數的關係。</p><p>你將sigmoid代入Cost function 畫出來就是下圖的左邊那樣。</p><p>右邊那張才是我們期望的，這樣才會保證收斂到Global minimum。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421997177049_c1.png'/></p><p></p><p>上面談到的convex function超出ML的範疇，有興趣的話可以去看MIT</p><p>的開放課程&nbsp; <a href='http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-253-convex-analysis-and-optimization-spring-2012/index.htm'/>Convex analysis and optimization</a>，或是線性規劃的課程。</p><p></p><p><b>簡化成本函數與梯度下降法Gradient Descent</b></p><p>下圖為邏輯式回歸的cost function，可以將兩個條件式合併為同一個。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422000348722_c1.png'/></p><p>你會想問說？why cost function 一定樣寫成這種形式？</p><p>事實上，這個式子是根據統計學中迴歸分析裡要做參數估測的其中一個方法-Maximum likelihood estimation 中文譯為最大相似度估測，另外兩個分別是Moment動差法也有人翻成矩估測法，以及 Ordinary least square estimation最小平方法。</p><ul class="comment"><li>啊！我沒學過統計學，所以這部分不是很熟，大致上了解即可。有時間還是學習一下好了。</li></ul>

<p>簡化cost function後，我們希望能將cost function最小化。</p><p>這時就需要用到梯度下降法。實際作法：用自己不斷地減去自己，直到收斂為止。</p><p>下圖Repeat的地方alpha後的那項，原本為logistic regression cost function，經過微分後，得到那個式子。這看起跟Linear regression 的梯度下降規則好像一樣咦？<i>(alpha為learning rate)</i></p><p>當然不同，別忘了Linear regression 與Logistic 的hypothesis定義是不一樣的喔！！</p><p>&nbsp;<img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422120287213_c1.png'/></p><p>另外，前面談到Linear regression時，有提feature scaling 能夠使梯度下降法更快的收斂。</p><p>在Logistic regression一樣適用。</p><p>總結：邏輯式迴歸是一個強大的分類方法。</p><p></p><p><b>進階優化</b></p><p>談到要minimum cost function，梯度下降法並不是唯一的方法。</p><p>還有其他優化的方式:</p><ul><li>Conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS</li></ul>
<p>他們的優點就是不必手動選擇learning rate、且跟梯度下降法來比的話會比較快收斂。缺點要說的話就是比較複雜。</p><p>這裡有點超出範疇，不需著墨太多，會使用就好了。</p><p>下圖右方，我們自己先定義好CostFunction，然後這個function會回傳cost function的長相，及梯度值為何。</p><p>接著呼叫Octave 裡 fminunc()這個函式來幫助你找到最佳的theta值。</p><p>options=optimset(.....)表示你可以對這個函式做一些結構上的設定</p><p>像是：給定梯度值、最大迭代的次數。</p><p>exitFlag=1 表示式子收斂</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422252887810_c1.png'/></p><p></p><p><b>分成多類問題：一對多</b></p><p>如下圖舉的列子</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422257290582_c1.png'/></p><p></p><p>要解決這類的問題，會用到one vs all方法</p><p>假定要分三類，就先將其中一類標註為正樣本，其他兩類則是標為負樣本，這樣即可找出第一類的分類器。其他兩類的分類器亦是如此。</p><p>所以，training 完之後會得到三個分類器。當有一筆新的data x進來，我們就可以藉由條件機率來判定為哪一類。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422257501214_c1.png'/></p><p></p><p></p><h2>Lecture VII Regularization (正規化)</h2><p>先談到一個很常遇到且非常重要的問題－overfitting 過度擬合</p><p>以下圖舉的例子，最左邊我們用一條直線來擬合原始的資料，可以明顯的看出效果會不好，這種情況我們稱作underfitting 擬合不足，也表示有high bias (高偏差) 的特性。</p><p>而右邊那張用到四次多項式來fitting原本的資料。</p><p>你可能會說這明明就很好呀！原始資料training時，確實可以得到較高的準確度，但當有新資料進來時，就容易預測失敗。這表示你選的model<b>泛化能力(Generlize)</b>不足，這種情況就稱overfitting，也表示有high variance的特性。</p><p>選擇上還是以中間的最為適當。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422264492952_c1.png'/></p><p>當遇到overfitting該如何解決?</p><ol><li>減少特徵向量的維度</li><ul><li>&nbsp;人工檢查哪一些特徵是可以捨棄的</li>
<li>&nbsp;Model selection algorithm-是說能夠自動選取哪些特徵要保留，哪些是可以捨棄的。</li>
<li>這個作法有用，但捨棄某些特徵的同時，你也捨棄掉一些有用的訊息了</li></ul>
</ol>
<p>2.正規化</p><ul><li>保留所有的特徵，但去減少參數的權值 (小化參數)</li>
<li>每個特徵都能對預測產生一點影響，因此我們可以將特徵給定不同的權值。</li>
<li>這樣一來便可使曲線變得較smooth一點。</li></ul>

<p>要正規化必須從cost function著手</p><p>以下圖為例我們把cost function加入兩項，變成一個新的cost function。</p><p>既然是cost 就是要盡量最小化！</p><p>所以theta3、theta4當然會趨近於零。(懲罰這兩個項)</p><p>那麼圖形就會退化成比較平滑smooth，近似成二次式。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422270544631_c1.png'/></p><p>&nbsp;這背後的想法就是要去簡化原本的hypothesis，免於overfitting。</p><p>&nbsp;</p><p>有個問題-當你擁有更多個特徵，你如何知道哪一個特徵的貢獻度會比較小？也就是說，我們根本不知道要如何選參數、縮小參數的數目。</p><p>真正的作法，以線性迴歸的cost function為例，我們會額外加入一個正規項，這項能收縮每一個參數讓值變小。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422271541896_c1.png'/></p><p>Lambda值給的太大，其他參數項都會弱化，造成underfitting!</p><p>Lambda值要怎麼找，後續會在探討。</p><p></p><p><b>正規化邏輯式迴歸</b></p><p>這裡談到當logistic regression 加入正規項之後，cost function最佳化的實作方式。讀者可以回顧上一章進階優化的部份。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424762720966_c1.png'/></p><p>cost function 最佳化(找到最小值)的作法：不斷地對cost function的theta微分算梯度，直到收斂為止。每迭代一次就更新cost function。</p><p>下一章會為各位介紹更強大的<b>非線性非類器</b>－類神經網路。</p><p>看下一章之前， 先了解一下線性分類器有哪些？via wikipedia</p><p></p><p><b>線性分類器</b></p><p>在機器學習領域，分類的目標是指將具有相似特徵的對象聚集。而一個線性分類器則透過特徵的<a href='http://zh.wikipedia.org/wiki/%E7%B7%9A%E6%80%A7%E7%B5%84%E5%90%88'/>線性組合</a>來做出分類決定，以達到此種目的。對象的特徵通常被描述為特徵值，而在向量中則描述為特徵向量。</p><p></p><p>如果輸入的特徵向量是實數向量x，則輸出的分數為：</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424766355074_c1.png'/></p><p>其中是一個權重向量w，而<i>f</i>是一個函數，該函數可以通過預先定義的功能塊，映射兩個向量的內積，得到希望的輸出。權重向量是從帶標籤的訓練樣本集合中所學到的。</p><p>通常，&quot;f&quot;是個簡單函數，會將超過一定閾值的值對應到第一類，其它的值對應到第二類。一個比較複雜的&quot;f&quot;則可能將某個東西歸屬於某一類。</p><p>線性分類器通常應用於對分類速度有較高要求的情況下，特別是當為稀疏向量時。</p><p>&nbsp;</p><p><b>&nbsp;依找w的方法將線性分類器分兩類</b></p><p>第一種模型條件機率。這類的演算法包括：</p><ul><li>線性判別分析（LDA） --- 假設為高斯條件密度模型。</li>
<li>樸素貝葉斯分類器 --- 假設為條件獨立性假設模型。</li></ul>

<p>第二種方式則稱為判別模型（discriminative models），這種方法是試圖去最大化一個訓練集（training set）的輸出值。</p><p>在訓練的成本函數中有一個額外的項加入，可以容易地表示正則化。</p><p>例子包含：</p><ul><li>Logistic --- 的最大似然估計，其假設觀察到的訓練集是由一個依賴於分類器的輸出的二元模型所產生。</li>
<li>感知元（Perceptron） --- 一個試圖去修正在訓練集中遇到錯誤的演算法。</li>
<li>支持向量機 --- 一個試圖去最大化決策超平面和訓練集中的樣本間的邊界（margin）的演算法。</li></ul>

<h2>Lecture VIII Neural Networks:Representation</h2><p>為何要談神經網路？NN是現在解決許多機器學習問題的首選技術。</p><p>先以一個例子提出先前用多項式解非線性問題時，遇到的困境。</p><p>當特徵很多時<b>(現實問題中，特徵個數一定是很大的)</b>，多項式的項數也會急遽上升，使得feature space膨脹。</p><p>如果你把一些相關項刪掉，則會使分類的準確度下降。</p><p>如圖：只考慮所有單獨變數的二次項，則你的decision boundary 就會是橢圓形。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424768557208_c1.png'/></p><p>再舉圖像識別的例子</p><p>以像素值作為初始特徵(沒用的特徵)，50*50的灰階圖像，特徵維度就2500。如果用二次函數去解，會產生三百多萬個特徵值。</p><p>但類神經網路面對feature space很大時，卻能輕易搞定。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424769428739_c1.png'/></p><p></p><p><b>神經元與腦</b></p><p>人類想建立一個模仿腦學習運作的演算法。但是你想一想，我們人腦可以處理聽覺、視覺、觸覺等等。感覺腦子應該有一堆演算法及程序來處理這些事情。這時人們大膽提出一個假設：人腦是否可以只用一個學習的演算法去處理這些事情？於是科學家開始對人腦的感官皮質層(cortex)做實驗(神經重接實驗) ...，結果發現先前的假設是可行的。</p><p></p><p><b>神經網路的架構與計算處理的介紹</b></p><p>腦內神經結構圖：<img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424866373118_c1.png'/></p><p>每個神經元由Dendrite接收其他神經元的訊息，用Axon傳遞訊息。</p><p></p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424866849548_c1.png'/></p><p>除了Input layer之外，其他的神經元我們稱作Logistic unit，而每個unit就是一個sigmoid function 或稱激勵函數(activation function)。此外，input Layer通常會多加一個 bias unit，這個bias 一定都設為1。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424867380202_c1.png'/></p><p>上圖為一個三層的神經網路Layer 1 輸入層，Layer 3 輸出層，而中間那層負責運算的則稱作隱藏層。請注意：隱藏層可以有多層，不一定只有一層。同樣地，隱藏層也會給他一個bias unit。</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424868332958_c1.png'/></p><p>符號表示說明：</p><p><i>theta </i>表示weight，上標指的在第幾層;下標則表示相連的unit。</p><p>e.g.&nbsp;&nbsp;</p><p><img src='https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424868930699_c1.png'/></p><p>屬於Layer 1的 Weight，由第二層的第一個unit與第一層的零個bias unit所構成。另外，整個theta可以用一個權值矩陣來表示，如上上圖Layer 2與Lyaer 1之間，就對應到一個維度為3x4的權值矩陣。(cuz bias unit)</p><p></p><p>隱藏層的unit這裡用<i>a </i>來表示，以和 input Layer做區別。</p><p>上標表示層數;下標則是指第幾個。</p><p>Hidden Layer每一個 unit 的值怎麼來的？</p><p>就是由 input unit 和 weight 線性組合得到的。</p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p></body>
</html>
