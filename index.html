
<!-- saved from url=(0052)https://hector.hackpad.com/ep/pad/static/Dp5GEmGaPmf -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><div class="line-gutter-backdrop"></div><table><tbody><tr><td class="line-number" value="1"></td><td class="line-content"><span class="html-doctype">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;</span></td></tr><tr><td class="line-number" value="2"></td><td class="line-content"><span class="html-tag">&lt;html <span class="html-attribute-name">xmlns</span>="<span class="html-attribute-value">http://www.w3.org/1999/xhtml</span>" <span class="html-attribute-name">lang</span>="<span class="html-attribute-value">en</span>" <span class="html-attribute-name">xml:lang</span>="<span class="html-attribute-value">en</span>"&gt;</span></td></tr><tr><td class="line-number" value="3"></td><td class="line-content"><span class="html-tag">&lt;head&gt;</span></td></tr><tr><td class="line-number" value="4"></td><td class="line-content"><span class="html-tag">&lt;meta <span class="html-attribute-name">http-equiv</span>="<span class="html-attribute-value">Content-type</span>" <span class="html-attribute-name">content</span>="<span class="html-attribute-value">text/html; charset=utf-8</span>" /&gt;</span></td></tr><tr><td class="line-number" value="5"></td><td class="line-content"><span class="html-tag">&lt;meta <span class="html-attribute-name">http-equiv</span>="<span class="html-attribute-value">Content-Language</span>" <span class="html-attribute-name">content</span>="<span class="html-attribute-value">en-us</span>" /&gt;</span></td></tr><tr><td class="line-number" value="6"></td><td class="line-content"><span class="html-tag">&lt;meta <span class="html-attribute-name">name</span>="<span class="html-attribute-value">version</span>" <span class="html-attribute-name">content</span>="<span class="html-attribute-value">2908</span>"/&gt;</span></td></tr><tr><td class="line-number" value="7"></td><td class="line-content"><span class="html-tag">&lt;style&gt;</span>body {font-family:Helvetica}ul.comment{list-style-image:url('https://hackpad.com/static/img/comment.png');} ul.task{list-style-image:url('https://hackpad.com/static/img/unchecked.png');}ul.taskdone{list-style-image:url('https://hackpad.com/static/img/checked.png');} <span class="html-tag">&lt;/style&gt;</span><span class="html-tag">&lt;title&gt;</span>/21841$Dp5GEmGaPmf<span class="html-tag">&lt;/title&gt;</span></td></tr><tr><td class="line-number" value="8"></td><td class="line-content"><span class="html-tag">&lt;/head&gt;</span></td></tr><tr><td class="line-number" value="9"></td><td class="line-content"><span class="html-tag">&lt;body&gt;</span><span class="html-tag">&lt;h1&gt;</span>Stanford Machine Learning by Andrew Ng on Coursera<span class="html-tag">&lt;/h1&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;h2&gt;</span>Lecture I 介紹.<span class="html-tag">&lt;/h2&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>What is machine learning?<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>1998s 卡內基梅隆的Tom Mitchell 對ML所下的定義：<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>一個computer program被認為能從<span class="html-tag">&lt;b&gt;</span><span class="html-tag">&lt;i&gt;</span>經驗E <span class="html-tag">&lt;/i&gt;</span><span class="html-tag">&lt;/b&gt;</span>中學習，解決<span class="html-tag">&lt;b&gt;</span><span class="html-tag">&lt;i&gt;</span>任務T<span class="html-tag">&lt;/i&gt;</span><span class="html-tag">&lt;/b&gt;</span>，達到一個<span class="html-tag">&lt;b&gt;</span><span class="html-tag">&lt;i&gt;</span>效能P <span class="html-tag">&lt;/i&gt;</span><span class="html-tag">&lt;/b&gt;</span>。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>從關於Ｔ中去學習Ｅ，之後用這個Ｅ<span class="html-tag">&lt;i&gt;</span>(Learning model) <span class="html-tag">&lt;/i&gt;</span>去執行Ｔ會得到效能值Ｐ，如果Ｐ不理想的話，則可以想辦法去改善Ｅ。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>區分machine learning algorithms<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;ul&gt;</span><span class="html-tag">&lt;li&gt;</span>Supervised 監督式學習<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="10"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Unsupervised 非監督式學習<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="11"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>Others: Reinforcement learning(加強式學習)、recommender systems(推薦系統)<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="12"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="13"></td><td class="line-content"><span class="html-tag">&lt;h2&gt;</span>Lecture V Octave Tutorial<span class="html-tag">&lt;/h2&gt;</span><span class="html-tag">&lt;p&gt;</span>Octave 是GNU的一套數學軟體類似 Matlab，不過Matlab是要錢的;Octave則是開源軟體。語法基本上與matlab相同，但兩者的toolbox並不相容。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>你會問說怎麼不用programming Language like: C++ 、Java、python<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>來實作演算法呢？<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>那是因為據以往經驗，你必需要對語言非常熟悉，否則在實作的過程中，常常會陷在一些在coding上的錯誤，一直除錯。這樣一來便失去學習機器學習演算法的意義了。因此，我們選一個所謂的prototype language 來實作演算法時，會比較好上手也比較省時間。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>往後在開發有關機器學習的應用時，不彷也先用octave或matlab built 出一個prototype測試看看可行性，覺得沒問題再用programming language 去完成他。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;ul <span class="html-attribute-name">class</span>="<span class="html-attribute-value">comment</span>"&gt;</span><span class="html-tag">&lt;li&gt;</span>這一章基本上都是在講授Octave的基本使用方式，有興趣自行研究。<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="14"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="15"></td><td class="line-content"><span class="html-tag">&lt;ul <span class="html-attribute-name">class</span>="<span class="html-attribute-value">comment</span>"&gt;</span><span class="html-tag">&lt;li&gt;</span>GNU是一堆自由軟體的集成，由Richard stallman提出的<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="16"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="17"></td><td class="line-content"><span class="html-tag">&lt;h2&gt;</span>Lecture VI Logistic Regression(邏輯式回歸)<span class="html-tag">&lt;/h2&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>Classification 分類問題<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;ul&gt;</span><span class="html-tag">&lt;li&gt;</span>&amp;nbsp; 二元分類 y={0,1}&amp;nbsp; 通常假定1為positive 正樣本&amp;nbsp;&amp;nbsp;&amp;nbsp; 0為negative 負樣本<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="18"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>&amp;nbsp; 多分類 y={0,1,2,3....}&amp;nbsp;<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="19"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="20"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>以分類是否為惡性腫瘤為例<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421830674709_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421830674709_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>看起來似乎用<span class="html-tag">&lt;b&gt;</span>Linear regression<span class="html-tag">&lt;/b&gt;</span>是可得到不錯的結果，真的嗎？<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>那假如我又得到一筆新的資料，下圖最右邊的x，會使得我們得到一個不好的 hypothesis。所以通常不要用線性回歸去解分類的問題。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421831636892_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421831636892_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>且用線性回歸去解分類問題，會造成一個現象<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>你要輸出的 y = 0 or 1<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>但是hypothesis 吐出來的結果可能會 &amp;gt;1 or &amp;lt;0 ，這顯然有問題。<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>為了解決這個現象，有人提出了<span class="html-tag">&lt;b&gt;</span>Logistic Regression<span class="html-tag">&lt;/b&gt;</span>邏輯回歸演算法<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>請注意：這是一個解決<span class="html-tag">&lt;b&gt;</span>分類問題<span class="html-tag">&lt;/b&gt;</span>的算法，別被回歸這個字給影響。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>假設之表示式 Hypothesis Representation&amp;nbsp;<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>Hypothesis 為 Sigmoid function or Logistic function<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>如圖右下方 當z值趨近正無限大 y會非常的接近1;反之則接近0。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421843990526_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421843990526_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>Hypothesis輸出的正確解釋為這樣<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>當你給定一個特徵向量x=[ x1 , x2]=[ 1 , tumorSize] 則 <span class="html-tag">&lt;i&gt;</span>h( X) =0.7<span class="html-tag">&lt;/i&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>這告訴我們病患有70％的機率為惡性腫瘤<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>如果用正規數學式來表示的話，就是用條件機率<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>給定x的條件下y = 1的機率式多少？<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>你會想問why？為什麼條件機率有關？<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>前面粗體的地方提到<span class="html-tag">&lt;b&gt;</span>你要輸出的 y = 0 or 1&amp;nbsp;<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>但是用線性回歸的hypothesis 吐出來的結果可能會 &amp;gt;1 or &amp;lt;0&amp;nbsp;<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>因此，用機率可以讓出來的值介於0～1之間，了解否？<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421845471381_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421845471381_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>決策邊界 Decision boundary&amp;nbsp;<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>下圖右邊polynomial hypothesis的參數theta先假定已經擬合好，<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>分別為 -3, 1 , 1<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>到目前為止，我們尚未談到如何去fitting(擬合)模型中的參數，下一段視頻我們會談到。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>這些參數的選擇是為了讓我們試著找出hypothesis何時將預測y=1,何時=0。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>圖中不等式將-3移向，得到藍筆的部份。告訴我們當你輸入的data滿足藍色不等式的時候，就會預測y=1。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>我們將藍筆不等式改寫成粉紅色的等式，很顯然這是一條直線方程式。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>也就是所謂的Decision boundary。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421979310572_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421979310572_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;ul <span class="html-attribute-name">style</span>="<span class="html-attribute-value">list-style: none;</span>"&gt;</span><span class="html-tag">&lt;li&gt;</span>( 圖：o為負樣本&amp;nbsp; x為正樣本 )<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="21"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>這裡有一點要注意的是<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>Decision boundary 是這個hypothesis的一個性質property，與原始的data無關。<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>也就是說，當給定的data與上圖的分佈是不同情況時，我們就需要去修改hypothesis的參數－參數擬合的問題。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>e.g.下圖data的分佈情況，不為線性可分。所以得修改hypothesis，找出合適的Decision boundary。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>可以明顯的看出Decision boundary為一個單位圓。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421982264614_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421982264614_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>再次強調我們不是用training set來定義Decision boundary，而是用training set來擬合(fitting) 參數theta。一旦找到這些參數，他就決定了Decision boundary。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>當然啦！你的data分佈可能會有更複雜的情況，會需要用到更高階的多項式來做為hypothesis，出來的Decision boundary可能會是橢圓、甚至其他有趣的圖形。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>成本函數或代價函數 cost function<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>顧名思義要在hypothesis 預測出的結果h(x) 與實際結果y之間找出要付出的成本是多少。以線性回歸來講，他的cost function即為兩者差值取平方。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>這裡我們要做的就是盡可能讓cost function最小化。假使我們用兩者差值取平方來做為邏輯式回歸的cost function會使得參數theta造出來的function為非凸函數non-convex。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>什麼是非凸函數呢？簡單講他不會是一個單調遞增的函數。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>而且在非凸函數上使用梯度下降法，我們無法保證一定能找到Global minimum，這也是sigmoid本身就是一個複雜的非線性函數的關係。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>你將sigmoid代入Cost function 畫出來就是下圖的左邊那樣。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>右邊那張才是我們期望的，這樣才會保證收斂到Global minimum。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421997177049_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1421997177049_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>上面談到的convex function超出ML的範疇，有興趣的話可以去看MIT<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>的開放課程&amp;nbsp; <span class="html-tag">&lt;a <span class="html-attribute-name">href</span>='<a class="html-attribute-value html-external-link" target="_blank" href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-253-convex-analysis-and-optimization-spring-2012/index.htm">http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-253-convex-analysis-and-optimization-spring-2012/index.htm</a>'/&gt;</span>Convex analysis and optimization<span class="html-tag">&lt;/a&gt;</span>，或是線性規劃的課程。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>簡化成本函數與梯度下降法Gradient Descent<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>下圖為邏輯式回歸的cost function，可以將兩個條件式合併為同一個。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422000348722_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422000348722_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>你會想問說？why cost function 一定樣寫成這種形式？<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>事實上，這個式子是根據統計學中迴歸分析裡要做參數估測的其中一個方法-Maximum likelihood estimation 中文譯為最大相似度估測，另外兩個分別是Moment動差法也有人翻成矩估測法，以及 Ordinary least square estimation最小平方法。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;ul <span class="html-attribute-name">class</span>="<span class="html-attribute-value">comment</span>"&gt;</span><span class="html-tag">&lt;li&gt;</span>啊！我沒學過統計學，所以這部分不是很熟，大致上了解即可。有時間還是學習一下好了。<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="22"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="23"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>簡化cost function後，我們希望能將cost function最小化。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>這時就需要用到梯度下降法。實際作法：用自己不斷地減去自己，直到收斂為止。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>下圖Repeat的地方alpha後的那項，原本為logistic regression cost function，經過微分後，得到那個式子。這看起跟Linear regression 的梯度下降規則好像一樣咦？<span class="html-tag">&lt;i&gt;</span>(alpha為learning rate)<span class="html-tag">&lt;/i&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>當然不同，別忘了Linear regression 與Logistic 的hypothesis定義是不一樣的喔！！<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>&amp;nbsp;<span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422120287213_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422120287213_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>另外，前面談到Linear regression時，有提feature scaling 能夠使梯度下降法更快的收斂。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>在Logistic regression一樣適用。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>總結：邏輯式迴歸是一個強大的分類方法。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>進階優化<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>談到要minimum cost function，梯度下降法並不是唯一的方法。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>還有其他優化的方式:<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;ul&gt;</span><span class="html-tag">&lt;li&gt;</span>Conjugate gradient<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="24"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>BFGS<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="25"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>L-BFGS<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="26"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>他們的優點就是不必手動選擇learning rate、且跟梯度下降法來比的話會比較快收斂。缺點要說的話就是比較複雜。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>這裡有點超出範疇，不需著墨太多，會使用就好了。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>下圖右方，我們自己先定義好CostFunction，然後這個function會回傳cost function的長相，及梯度值為何。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>接著呼叫Octave 裡 fminunc()這個函式來幫助你找到最佳的theta值。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>options=optimset(.....)表示你可以對這個函式做一些結構上的設定<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>像是：給定梯度值、最大迭代的次數。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>exitFlag=1 表示式子收斂<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422252887810_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422252887810_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>分成多類問題：一對多<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>如下圖舉的列子<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422257290582_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422257290582_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>要解決這類的問題，會用到one vs all方法<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>假定要分三類，就先將其中一類標註為正樣本，其他兩類則是標為負樣本，這樣即可找出第一類的分類器。其他兩類的分類器亦是如此。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>所以，training 完之後會得到三個分類器。當有一筆新的data x進來，我們就可以藉由條件機率來判定為哪一類。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422257501214_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422257501214_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;h2&gt;</span>Lecture VII Regularization (正規化)<span class="html-tag">&lt;/h2&gt;</span><span class="html-tag">&lt;p&gt;</span>先談到一個很常遇到且非常重要的問題－overfitting 過度擬合<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>以下圖舉的例子，最左邊我們用一條直線來擬合原始的資料，可以明顯的看出效果會不好，這種情況我們稱作underfitting 擬合不足，也表示有high bias (高偏差) 的特性。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>而右邊那張用到四次多項式來fitting原本的資料。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>你可能會說這明明就很好呀！原始資料training時，確實可以得到較高的準確度，但當有新資料進來時，就容易預測失敗。這表示你選的model<span class="html-tag">&lt;b&gt;</span>泛化能力(Generlize)<span class="html-tag">&lt;/b&gt;</span>不足，這種情況就稱overfitting，也表示有high variance的特性。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>選擇上還是以中間的最為適當。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422264492952_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422264492952_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>當遇到overfitting該如何解決?<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;ol&gt;</span><span class="html-tag">&lt;li&gt;</span>減少特徵向量的維度<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;ul&gt;</span><span class="html-tag">&lt;li&gt;</span>&amp;nbsp;人工檢查哪一些特徵是可以捨棄的<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="27"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>&amp;nbsp;Model selection algorithm-是說能夠自動選取哪些特徵要保留，哪些是可以捨棄的。<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="28"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>這個作法有用，但捨棄某些特徵的同時，你也捨棄掉一些有用的訊息了<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="29"></td><td class="line-content"><span class="html-tag">&lt;/ol&gt;</span></td></tr><tr><td class="line-number" value="30"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>2.正規化<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;ul&gt;</span><span class="html-tag">&lt;li&gt;</span>保留所有的特徵，但去減少參數的權值 (小化參數)<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="31"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>每個特徵都能對預測產生一點影響，因此我們可以將特徵給定不同的權值。<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="32"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>這樣一來便可使曲線變得較smooth一點。<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="33"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="34"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>要正規化必須從cost function著手<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>以下圖為例我們把cost function加入兩項，變成一個新的cost function。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>既然是cost 就是要盡量最小化！<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>所以theta3、theta4當然會趨近於零。(懲罰這兩個項)<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>那麼圖形就會退化成比較平滑smooth，近似成二次式。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422270544631_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422270544631_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>&amp;nbsp;這背後的想法就是要去簡化原本的hypothesis，免於overfitting。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>&amp;nbsp;<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>有個問題-當你擁有更多個特徵，你如何知道哪一個特徵的貢獻度會比較小？也就是說，我們根本不知道要如何選參數、縮小參數的數目。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>真正的作法，以線性迴歸的cost function為例，我們會額外加入一個正規項，這項能收縮每一個參數讓值變小。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422271541896_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1422271541896_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>Lambda值給的太大，其他參數項都會弱化，造成underfitting!<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>Lambda值要怎麼找，後續會在探討。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>正規化邏輯式迴歸<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>這裡談到當logistic regression 加入正規項之後，cost function最佳化的實作方式。讀者可以回顧上一章進階優化的部份。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424762720966_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424762720966_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>cost function 最佳化(找到最小值)的作法：不斷地對cost function的theta微分算梯度，直到收斂為止。每迭代一次就更新cost function。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>下一章會為各位介紹更強大的<span class="html-tag">&lt;b&gt;</span>非線性非類器<span class="html-tag">&lt;/b&gt;</span>－類神經網路。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>看下一章之前， 先了解一下線性分類器有哪些？via wikipedia<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>線性分類器<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>在機器學習領域，分類的目標是指將具有相似特徵的對象聚集。而一個線性分類器則透過特徵的<span class="html-tag">&lt;a <span class="html-attribute-name">href</span>='<a class="html-attribute-value html-external-link" target="_blank" href="http://zh.wikipedia.org/wiki/%E7%B7%9A%E6%80%A7%E7%B5%84%E5%90%88">http://zh.wikipedia.org/wiki/%E7%B7%9A%E6%80%A7%E7%B5%84%E5%90%88</a>'/&gt;</span>線性組合<span class="html-tag">&lt;/a&gt;</span>來做出分類決定，以達到此種目的。對象的特徵通常被描述為特徵值，而在向量中則描述為特徵向量。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>如果輸入的特徵向量是實數向量x，則輸出的分數為：<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424766355074_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424766355074_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>其中是一個權重向量w，而<span class="html-tag">&lt;i&gt;</span>f<span class="html-tag">&lt;/i&gt;</span>是一個函數，該函數可以通過預先定義的功能塊，映射兩個向量的內積，得到希望的輸出。權重向量是從帶標籤的訓練樣本集合中所學到的。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>通常，&amp;quot;f&amp;quot;是個簡單函數，會將超過一定閾值的值對應到第一類，其它的值對應到第二類。一個比較複雜的&amp;quot;f&amp;quot;則可能將某個東西歸屬於某一類。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>線性分類器通常應用於對分類速度有較高要求的情況下，特別是當為稀疏向量時。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>&amp;nbsp;<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>&amp;nbsp;依找w的方法將線性分類器分兩類<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>第一種模型條件機率。這類的演算法包括：<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;ul&gt;</span><span class="html-tag">&lt;li&gt;</span>線性判別分析（LDA） --- 假設為高斯條件密度模型。<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="35"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>樸素貝葉斯分類器 --- 假設為條件獨立性假設模型。<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="36"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="37"></td><td class="line-content"><span class="html-tag">&lt;p&gt;</span>第二種方式則稱為判別模型（discriminative models），這種方法是試圖去最大化一個訓練集（training set）的輸出值。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>在訓練的成本函數中有一個額外的項加入，可以容易地表示正則化。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>例子包含：<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;ul&gt;</span><span class="html-tag">&lt;li&gt;</span>Logistic --- 的最大似然估計，其假設觀察到的訓練集是由一個依賴於分類器的輸出的二元模型所產生。<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="38"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>感知元（Perceptron） --- 一個試圖去修正在訓練集中遇到錯誤的演算法。<span class="html-tag">&lt;/li&gt;</span></td></tr><tr><td class="line-number" value="39"></td><td class="line-content"><span class="html-tag">&lt;li&gt;</span>支持向量機 --- 一個試圖去最大化決策超平面和訓練集中的樣本間的邊界（margin）的演算法。<span class="html-tag">&lt;/li&gt;</span><span class="html-tag">&lt;/ul&gt;</span></td></tr><tr><td class="line-number" value="40"></td><td class="line-content"><br></td></tr><tr><td class="line-number" value="41"></td><td class="line-content"><span class="html-tag">&lt;h2&gt;</span>Lecture VIII Neural Networks:Representation<span class="html-tag">&lt;/h2&gt;</span><span class="html-tag">&lt;p&gt;</span>為何要談神經網路？NN是現在解決許多機器學習問題的首選技術。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>先以一個例子提出先前用多項式解非線性問題時，遇到的困境。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>當特徵很多時<span class="html-tag">&lt;b&gt;</span>(現實問題中，特徵個數一定是很大的)<span class="html-tag">&lt;/b&gt;</span>，多項式的項數也會急遽上升，使得feature space膨脹。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>如果你把一些相關項刪掉，則會使分類的準確度下降。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>如圖：只考慮所有單獨變數的二次項，則你的decision boundary 就會是橢圓形。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424768557208_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424768557208_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>再舉圖像識別的例子<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>以像素值作為初始特徵(沒用的特徵)，25*25的灰階圖像，特徵維度就2500。如果用二次函數去解，會產生三百多萬個特徵值。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>但類神經網路面對feature space很大時，卻能輕易搞定。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;img <span class="html-attribute-name">src</span>='<a class="html-attribute-value html-resource-link" target="_blank" href="https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424769428739_c1.png">https://hackpad-attachments.s3.amazonaws.com/hackpad.com_Dp5GEmGaPmf_p.286838_1424769428739_c1.png</a>'/&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;b&gt;</span>神經元與腦<span class="html-tag">&lt;/b&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span>人類想建立一個模仿腦學習運作的演算法。但是你想一想，我們人腦可以處理聽覺、視覺、觸覺等等。感覺腦子應該有一堆演算法及程序來處理這些事情。這時人們大膽提出一個假設：人腦是否可以只用一個學習的演算法去處理這些事情？於是科學家開始對人腦的感官皮質層cortex做實驗，又稱為神經重接實驗。<span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;p&gt;</span><span class="html-tag">&lt;/p&gt;</span><span class="html-tag">&lt;/body&gt;</span></td></tr><tr><td class="line-number" value="42"></td><td class="line-content"><span class="html-tag">&lt;/html&gt;</span></td></tr><tr><td class="line-number" value="43"></td><td class="line-content"><span class="html-end-of-file"></span></td></tr></tbody></table></body></html>